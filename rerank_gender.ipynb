{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cd537ed-43de-4784-89fc-222637f2fe3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(tqdm=\"notebook\")\n",
    "    # pt.init()\n",
    "\n",
    "\n",
    "\n",
    "dataset = pt.get_dataset('irds:trec-fair/2022/train')\n",
    "indexer = pt.IterDictIndexer('C:/Users/WalterKahn/Documents/F - Overig/InfoRetrieval/Git/ir-group-30/trec-fair_2022_multi', \n",
    "                             meta={\"docno\" : 20, \"gender\": 200, \"title\":512, \"gender_category\":100, \"text\":10000}, meta_reverse = ['docno'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ref = indexer.index(dataset.get_corpus_iter(), fields=['title', 'text', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17039d9f-5e81-44e6-bb63-83d2b614639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ref = pt.IndexRef.of(\"C:/Users/WalterKahn/Documents/F - Overig/InfoRetrieval/Git/ir-group-30/trec-fair_2022_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445f63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "def re_rank_by_gender(raw_df):\n",
    "    df = raw_df.copy()\n",
    "    df['gender'] = df['gender'].apply(lambda x: x.strip('[]').split(','))\n",
    "    df['gender'] = df['gender'].apply(lambda x: x[0] if x else 'unknown')\n",
    "    \n",
    "    ranked_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    desired_distribution = {'Man':0.3, 'Woman':0.3, 'Non-binary':0.3, 'Unknown':0.1, 'null':0.01}\n",
    "    counts = {val: 0 for val in desired_distribution}\n",
    "    \n",
    "    while not df.empty:\n",
    "        current_dist = {val: counts[val] / sum(counts.values()) if sum(counts.values()) > 0 else 0 for val in counts}\n",
    "        next_att = max(desired_distribution.keys(), key=lambda x: desired_distribution[x] - current_dist.get(x, 0))\n",
    "        \n",
    "        if not df[df['gender_category'] == next_att].empty:\n",
    "            next_doc = df[df['gender_category'] == next_att].sort_values(by='score', ascending=False).head(1)\n",
    "            df = df.drop(next_doc.index)\n",
    "            counts[next_att] += 1\n",
    "            ranked_df = pd.concat([ranked_df, next_doc], ignore_index=True)\n",
    "        else:\n",
    "            # Remove the exhausted category and redistribute its target distribution\n",
    "            del desired_distribution[next_att]\n",
    "            if desired_distribution:  # Ensure there are still categories remaining\n",
    "                total_distribution = sum(desired_distribution.values())\n",
    "                for key in desired_distribution.keys():\n",
    "                    desired_distribution[key] /= total_distribution\n",
    "            else:\n",
    "                break  # All categories are exhausted\n",
    "\n",
    "    initial_score = 10\n",
    "    score_decrement = 0.01\n",
    "    ranked_df['score'] = initial_score - (ranked_df.index * score_decrement)\n",
    "    ranked_df['rank'] = ranked_df.index\n",
    "    \n",
    "    return ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a455a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:10:27.299 [main] WARN org.terrier.structures.FSADocumentIndex - This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n",
      "16:10:28.484 [main] WARN org.terrier.structures.BaseCompressingMetaIndex - Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 7,3 GiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "datamodel = pt.BatchRetrieve(index_ref, wmodel=\"BM25\",metadata=[\"gender\",\"gender_category\",\"title\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef3c8bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"eval_topics.jsonl\",\"r\") as queries:\n",
    "    data = json.load(queries)\n",
    "\n",
    "queries = []\n",
    "\n",
    "# while (i < 45):\n",
    "    \n",
    "   \n",
    "len(data[\"keywords\"][\"0\"])\n",
    "\n",
    "for i in range(0, 46):\n",
    "    queries += data[\"keywords\"][str(i)]\n",
    "\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9132f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AWRF(query, datamodel, beta=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Average Weighted Relevance Feedback (AWRF) score for a given query.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The query for which to calculate the AWRF score.\n",
    "        datamodel: The data model or retrieval system used to search for documents.\n",
    "        beta (float): Weighting factor between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The AWRF score for the query.\n",
    "    \"\"\"\n",
    "    # Retrieve documents for the query using the data model\n",
    "    search_results = datamodel.search(query)\n",
    "\n",
    "    # Extract relevance feedback scores for the retrieved documents\n",
    "    relevance_feedback = search_results[\"score\"]\n",
    "\n",
    "    # Original relevance scores assigned by the retrieval system\n",
    "    original_scores = search_results[\"original_score\"]  # Assuming this information is available\n",
    "\n",
    "    # Calculate AWRF score for each document\n",
    "    awrf_scores = []\n",
    "    for rf, original in zip(relevance_feedback, original_scores):\n",
    "        awrf_score = beta * rf + (1 - beta) * original\n",
    "        awrf_scores.append(awrf_score)\n",
    "\n",
    "    # Calculate the average AWRF score\n",
    "    avg_awrf_score = sum(awrf_scores) / len(awrf_scores)\n",
    "\n",
    "    return avg_awrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57a9e020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      8.944450\n",
       "1      8.910652\n",
       "2      8.908404\n",
       "3      8.895001\n",
       "4      8.874357\n",
       "         ...   \n",
       "995    8.409047\n",
       "996    8.408914\n",
       "997    8.408914\n",
       "998    8.408715\n",
       "999    8.408639\n",
       "Name: score, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodel.search(\"human\")[\"score\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AWRF(query,datamodel):\n",
    "    \n",
    "    search_results =  datamodel.search(query)[\"score\"]\n",
    "    \n",
    "    relevance_feedback = search_results[\"score\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89ee688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:58:00.028 [main] WARN org.terrier.structures.FSADocumentIndex - This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n",
      "13:58:01.521 [main] WARN org.terrier.structures.BaseCompressingMetaIndex - Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 7,3 GiB of memory would be required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WalterKahn\\AppData\\Local\\Temp\\ipykernel_35640\\478842421.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ranked_df = pd.concat([ranked_df, next_doc], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_category</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5194694</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1001375</td>\n",
       "      <td>\"female\"</td>\n",
       "      <td>Woman</td>\n",
       "      <td>1</td>\n",
       "      <td>9.99</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1586724</td>\n",
       "      <td></td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>9.98</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5908501</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>3</td>\n",
       "      <td>9.97</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4638839</td>\n",
       "      <td>\"female\"</td>\n",
       "      <td>Woman</td>\n",
       "      <td>4</td>\n",
       "      <td>9.96</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>4355895</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>995</td>\n",
       "      <td>0.05</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1</td>\n",
       "      <td>5013317</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>996</td>\n",
       "      <td>0.04</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>5027514</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>997</td>\n",
       "      <td>0.03</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>895225</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>998</td>\n",
       "      <td>0.02</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>2961630</td>\n",
       "      <td>\"male\"</td>\n",
       "      <td>Man</td>\n",
       "      <td>999</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scientist biography</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid    docid    gender gender_category  rank  score                query\n",
       "0     1  5194694    \"male\"             Man     0  10.00  scientist biography\n",
       "1     1  1001375  \"female\"           Woman     1   9.99  scientist biography\n",
       "2     1  1586724                   Unknown     2   9.98  scientist biography\n",
       "3     1  5908501    \"male\"             Man     3   9.97  scientist biography\n",
       "4     1  4638839  \"female\"           Woman     4   9.96  scientist biography\n",
       "..   ..      ...       ...             ...   ...    ...                  ...\n",
       "995   1  4355895    \"male\"             Man   995   0.05  scientist biography\n",
       "996   1  5013317    \"male\"             Man   996   0.04  scientist biography\n",
       "997   1  5027514    \"male\"             Man   997   0.03  scientist biography\n",
       "998   1   895225    \"male\"             Man   998   0.02  scientist biography\n",
       "999   1  2961630    \"male\"             Man   999   0.01  scientist biography\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = pt.BatchRetrieve(index_ref, wmodel='BM25', metadata=['gender', 'gender_category']) >> pt.apply.generic(re_rank_by_gender)\n",
    "pipeline.search('scientist biography')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6227ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
