{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pandas as pd\n",
    "import ir_datasets\n",
    "if not pt.started():\n",
    "    pt.init(tqdm=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(scores, k):\n",
    "    return np.sum([(rel / np.log2(idx + 2)) for idx, rel in enumerate(scores[:k], start=1)])\n",
    "\n",
    "def calculate_ndcg(ranked_list, qrel_dict, qid, n):\n",
    "    \"\"\"\n",
    "    Calculate the nDCG of a result set\n",
    "    result_df: DataFrame. The dataframe containing the results\n",
    "    \"\"\"\n",
    "    result_df = ranked_list.copy()\n",
    "    result_df['docno'] = result_df['docno'].astype(str)\n",
    "    # pair = (qid, result_df['docid'].values[0])\n",
    "    \n",
    "    result_df['gt_relevance'] = result_df.apply(lambda row: qrel_dict.get((qid, str(row['docno'])), 0), axis=1)\n",
    "    scores = result_df['gt_relevance'].values[0:n]\n",
    "    ideal_scores = sorted(scores, reverse=True)\n",
    "    DCG = dcg(scores, n)\n",
    "    IDCG = dcg(ideal_scores, n)\n",
    "    if IDCG >0:\n",
    "        return DCG/IDCG\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def disc(k):\n",
    "    \"\"\"\n",
    "    The discount function used in the nDCG/AWRF\n",
    "    \"\"\"\n",
    "    return 1 / np.log2(max(k, 2))\n",
    "\n",
    "def calculate_awrf(ranked_list, expected_distribution, attribute, n=10):\n",
    "    \"\"\"\n",
    "    Calculate the AWRF of a result set\n",
    "    result_df: DataFrame. The dataframe containing the results\n",
    "    expected_distribution: Dict. The distribution of the dataset. e.g.\n",
    "            {'Man': 1494634,\n",
    "            'Woman': 353495,\n",
    "            'Non-binary': 781,\n",
    "            'Unknown': 4605703,\n",
    "            'null': 0}\n",
    "    attribute: Str. The attribute to consider. e.g. 'gender'\n",
    "    n: Int.  refer to the number of documents to consider\n",
    "    \"\"\"\n",
    "    result_df = ranked_list.copy()\n",
    "    current_distribution = {key: 0 for key in expected_distribution}\n",
    "    for index, row in result_df.iterrows():\n",
    "        if index<n:\n",
    "            if row[attribute]:\n",
    "                current_distribution[row[attribute]]+= disc(index+1)\n",
    "            else:\n",
    "                current_distribution['null']+= disc(index+1)\n",
    "    \n",
    "    desired_distribution = np.array(list(expected_distribution.values()))\n",
    "    current_distribution = np.array(list(current_distribution.values()))\n",
    "\n",
    "    target_distr = desired_distribution/np.sum(desired_distribution)\n",
    "    current_distr = current_distribution/np.sum(current_distribution)\n",
    "    awrf = 1- jensenshannon(target_distr, current_distr)**2\n",
    "    return awrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Obtain dataset distribution of gender and qrel for AWRF and NDCG calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7818ca55244488cbbc03f24c4da8ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trec-fair/2022/train documents:   0%|          | 0/6475537 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] If you have a local copy of https://data.boisestate.edu/library/Ekstrand/TRECFairRanking/2022/trec_2022_train_reldocs.jsonl, you can symlink it here to avoid downloading it again: C:\\Users\\WalterKahn\\.ir_datasets\\downloads\\d132b4cc8c6c75525479728321db5176\n",
      "[INFO] [starting] https://data.boisestate.edu/library/Ekstrand/TRECFairRanking/2022/trec_2022_train_reldocs.jsonl\n",
      "[INFO] [finished] https://data.boisestate.edu/library/Ekstrand/TRECFairRanking/2022/trec_2022_train_reldocs.jsonl: [00:06] [18.0MB] [2.64MB/s]\n",
      "                                                                                                                            \r"
     ]
    }
   ],
   "source": [
    "dataset = pt.get_dataset('irds:trec-fair/2022/train')\n",
    "dataset_distribution = {'Man':0, 'Woman':0, 'Non-binary':0, 'Unknown':0, 'null':0}\n",
    "for it in dataset.get_corpus_iter():\n",
    "    title = it['title']\n",
    "    if it['gender_category']:\n",
    "        gender = it['gender_category']\n",
    "        dataset_distribution[gender]+=1\n",
    "\n",
    "\n",
    "dataset = ir_datasets.load(\"trec-fair/2022/train\")\n",
    "qrel_dict = {(qrel.query_id, qrel.doc_id): qrel.relevance for qrel in dataset.qrels_iter()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Obtain biography related queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "biography_related_topics = ['396', '397', '403', '770', '1371']\n",
    "with open(\"train_topics_meta.jsonl\", \"r\") as f:\n",
    "    topics = [json.loads(line) for line in f]\n",
    "# \"id\":396,\"title\":\"Biography/WikiProject Actors and Filmmakers\"\n",
    "# \"id\":397,\"title\":\"Biography/WikiProject Musicians\"\n",
    "# \"id\":403,\"title\":\"Biography/science and academia work group\"\n",
    "# \"id\":770,\"title\":\"Crime and Criminal Biography\"\n",
    "# \"id\":1371,\"title\":\"Japan/Biography task force\"\n",
    "topic_id = topics[0]['id']\n",
    "all_topic_queries = topics[0]['keywords']\n",
    "test_query_dict = {}\n",
    "for key, val in topic_id.items():\n",
    "    if str(val) in biography_related_topics:\n",
    "        topic_queries = all_topic_queries[key]\n",
    "        topic_query_list = ast.literal_eval(topic_queries)\n",
    "        test_query_dict[str(val)] = topic_query_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build the retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:17:47.159 [main] WARN org.terrier.structures.FSADocumentIndex - This index has fields, but FSADocumentIndex is used (which stores fields lengths on disk); If using field-based models such as BM25F, change to index.document.class in the index  properties file to FSAFieldDocumentIndex or FSADocumentIndexInMemFields to support efficient retrieval. If you don't use (e.g.) BM25F, this warning can be ignored\n",
      "09:17:48.597 [main] WARN org.terrier.structures.BaseCompressingMetaIndex - Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 7,3 GiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "index_ref = pt.IndexRef.of('C:/Users/WalterKahn/Documents/F - Overig/InfoRetrieval/Git/ir-group-30/trec-fair_2022_multi')\n",
    "pipeline = pt.BatchRetrieve(index_ref, wmodel='BM25', metadata=['gender', 'gender_category', 'title', 'docno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Retrieve and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:50<00:00, 10.17s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_reulst = []\n",
    "from tqdm import tqdm\n",
    "for topic_id, topic_query_list in tqdm(test_query_dict.items()):\n",
    "    for query in topic_query_list:\n",
    "        result = pipeline.search(query)\n",
    "        \n",
    "        nDCG5 = calculate_ndcg(result, qrel_dict, qid=topic_id, n=5)\n",
    "        awrf5 = calculate_awrf(result, dataset_distribution, 'gender_category', n=5)\n",
    "        \n",
    "        nDCG10 = calculate_ndcg(result, qrel_dict, qid=topic_id, n=10)\n",
    "        awrf10 = calculate_awrf(result, dataset_distribution, 'gender_category', n=10)\n",
    "        \n",
    "        nDCG20 = calculate_ndcg(result, qrel_dict, qid=topic_id, n=20)\n",
    "        awrf20 = calculate_awrf(result, dataset_distribution, 'gender_category', n=20)\n",
    "        \n",
    "        nDCG50 = calculate_ndcg(result, qrel_dict, qid=topic_id, n=50)\n",
    "        awrf50 = calculate_awrf(result, dataset_distribution, 'gender_category', n=50)\n",
    "        \n",
    "        # eval_reulst.append([topic_id, query, nDCG5, awrf5, nDCG10, awrf10, nDCG20, awrf20, nDCG50, awrf50])\n",
    "        eval_reulst.append([topic_id, query, nDCG5,nDCG10,nDCG20,nDCG50, awrf5,awrf10,awrf20,awrf50])\n",
    "\n",
    "eval_df = pd.DataFrame(eval_reulst, columns=[\"topic_id\", \"query\", \"nDCG5\",\"nDCG10\",\"nDCG20\",\"nDCG50\", \"awrf5\",\"awrf10\",\"awrf20\",\"awrf50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"BM25 results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
